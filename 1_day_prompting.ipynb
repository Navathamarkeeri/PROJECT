{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Navathamarkeeri/PROJECT/blob/python/1_day_prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:19:13.123171Z",
          "iopub.execute_input": "2024-11-12T17:19:13.123635Z",
          "iopub.status.idle": "2024-11-12T17:19:13.13199Z",
          "shell.execute_reply.started": "2024-11-12T17:19:13.12359Z",
          "shell.execute_reply": "2024-11-12T17:19:13.130378Z"
        },
        "id": "MarBK37307ys"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U -q \"google-generativeai>=0.8.3\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:19:13.134214Z",
          "iopub.execute_input": "2024-11-12T17:19:13.134782Z",
          "iopub.status.idle": "2024-11-12T17:19:25.725739Z",
          "shell.execute_reply.started": "2024-11-12T17:19:13.134717Z",
          "shell.execute_reply": "2024-11-12T17:19:25.724208Z"
        },
        "id": "82UR_Bl707ys",
        "outputId": "cea938cd-7da4-4530-812f-f8f2108cd87e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Note: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from IPython.display import HTML, Markdown, display"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:19:25.728224Z",
          "iopub.execute_input": "2024-11-12T17:19:25.72867Z",
          "iopub.status.idle": "2024-11-12T17:19:25.735257Z",
          "shell.execute_reply.started": "2024-11-12T17:19:25.72862Z",
          "shell.execute_reply": "2024-11-12T17:19:25.733871Z"
        },
        "id": "fmuIVQS307yt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:19:25.736631Z",
          "iopub.execute_input": "2024-11-12T17:19:25.737173Z",
          "iopub.status.idle": "2024-11-12T17:19:25.838411Z",
          "shell.execute_reply.started": "2024-11-12T17:19:25.73713Z",
          "shell.execute_reply": "2024-11-12T17:19:25.837045Z"
        },
        "id": "Bl9y3pUT07yt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "flash = genai.GenerativeModel('gemini-1.5-flash')\n",
        "response = flash.generate_content(\"Explain AI to me like I'm a kid.\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:19:25.84165Z",
          "iopub.execute_input": "2024-11-12T17:19:25.842087Z",
          "iopub.status.idle": "2024-11-12T17:19:27.173193Z",
          "shell.execute_reply.started": "2024-11-12T17:19:25.842044Z",
          "shell.execute_reply": "2024-11-12T17:19:27.17164Z"
        },
        "id": "cYGoIW6U07yu",
        "outputId": "4e9a3e8a-7af2-45e6-c02c-5ed580e8a0ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Imagine you have a super smart friend who loves to learn new things. That friend is like an AI (Artificial Intelligence)! \n\nJust like your friend, an AI can:\n\n* **Learn from lots of information:** Like reading lots of books, an AI can learn from pictures, videos, and text. It can even learn by watching how you do things!\n* **Solve problems:** If you ask your friend a question, they can use what they've learned to figure out the answer. An AI can do the same, helping you with your homework or even playing games.\n* **Do things for you:** Your friend might help you clean up your room, right? An AI can also do tasks like making a playlist of songs you like or telling you the weather. \n\nBut unlike your friend, an AI doesn't have feelings or a body. It's just a computer program that's really good at learning and solving problems. \n\nThink of AI as a super helpful tool that can make our lives easier and more fun! \n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:19:27.174666Z",
          "iopub.execute_input": "2024-11-12T17:19:27.175098Z",
          "iopub.status.idle": "2024-11-12T17:19:27.184389Z",
          "shell.execute_reply.started": "2024-11-12T17:19:27.175055Z",
          "shell.execute_reply": "2024-11-12T17:19:27.183018Z"
        },
        "id": "xjpDHh6507yu",
        "outputId": "88903f5b-6efd-4a5d-d831-5b79e47d0e28"
      },
      "outputs": [
        {
          "execution_count": 97,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "Imagine you have a super smart friend who loves to learn new things. That friend is like an AI (Artificial Intelligence)! \n\nJust like your friend, an AI can:\n\n* **Learn from lots of information:** Like reading lots of books, an AI can learn from pictures, videos, and text. It can even learn by watching how you do things!\n* **Solve problems:** If you ask your friend a question, they can use what they've learned to figure out the answer. An AI can do the same, helping you with your homework or even playing games.\n* **Do things for you:** Your friend might help you clean up your room, right? An AI can also do tasks like making a playlist of songs you like or telling you the weather. \n\nBut unlike your friend, an AI doesn't have feelings or a body. It's just a computer program that's really good at learning and solving problems. \n\nThink of AI as a super helpful tool that can make our lives easier and more fun! \n"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "chat = flash.start_chat(history=[])\n",
        "response = chat.send_message('Hello! My name is Zlork.')\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:19:27.186225Z",
          "iopub.execute_input": "2024-11-12T17:19:27.18675Z",
          "iopub.status.idle": "2024-11-12T17:19:27.547648Z",
          "shell.execute_reply.started": "2024-11-12T17:19:27.186674Z",
          "shell.execute_reply": "2024-11-12T17:19:27.546303Z"
        },
        "id": "kU5hGetU07yu",
        "outputId": "92109c01-b6f2-4493-ab1d-59384fd68830"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Hello Zlork! It's nice to meet you. What can I do for you today? ðŸ˜Š \n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message('Can you tell something interesting about dinosaurs?')\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:19:27.549817Z",
          "iopub.execute_input": "2024-11-12T17:19:27.550187Z",
          "iopub.status.idle": "2024-11-12T17:19:28.30362Z",
          "shell.execute_reply.started": "2024-11-12T17:19:27.550151Z",
          "shell.execute_reply": "2024-11-12T17:19:28.302101Z"
        },
        "id": "xHYpntD607yv",
        "outputId": "53dc42f3-d491-4377-ca0a-494aae9ea32c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Did you know that the largest land animal that ever lived, *Argentinosaurus*, was a dinosaur?  This behemoth was about 100 feet long, about the length of three school buses!  That's just one of the many incredible things about dinosaurs. \n\nWhat kind of dinosaur facts are you interested in? Do you want to know about their extinction, their fascinating adaptations, or perhaps some of the weird and wonderful species that roamed the Earth? I'm happy to share! ðŸ¦– \n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# While you have the `chat` object around, the conversation state\n",
        "# persists. Confirm that by asking if it knows my name.\n",
        "response = chat.send_message('Do you remember what my name is?')\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:19:28.30542Z",
          "iopub.execute_input": "2024-11-12T17:19:28.305984Z",
          "iopub.status.idle": "2024-11-12T17:19:28.727082Z",
          "shell.execute_reply.started": "2024-11-12T17:19:28.305925Z",
          "shell.execute_reply": "2024-11-12T17:19:28.725782Z"
        },
        "id": "h2ky0ZX_07yv",
        "outputId": "e6adef67-6756-42b6-aabe-a8f48deec10b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Of course I do!  You are Zlork.  ðŸ˜Š  It's a unique and memorable name.  Is there anything else I can help you with today? \n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:19:28.728626Z",
          "iopub.execute_input": "2024-11-12T17:19:28.729166Z",
          "iopub.status.idle": "2024-11-12T17:19:28.794347Z",
          "shell.execute_reply.started": "2024-11-12T17:19:28.729111Z",
          "shell.execute_reply": "2024-11-12T17:19:28.793018Z"
        },
        "id": "nLBTJUeq07yv",
        "outputId": "a30dd1ba-c296-49ac-dced-2e231c60a238"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "models/chat-bison-001\nmodels/text-bison-001\nmodels/embedding-gecko-001\nmodels/gemini-1.0-pro-latest\nmodels/gemini-1.0-pro\nmodels/gemini-pro\nmodels/gemini-1.0-pro-001\nmodels/gemini-1.0-pro-vision-latest\nmodels/gemini-pro-vision\nmodels/gemini-1.5-pro-latest\nmodels/gemini-1.5-pro-001\nmodels/gemini-1.5-pro-002\nmodels/gemini-1.5-pro\nmodels/gemini-1.5-pro-exp-0801\nmodels/gemini-1.5-pro-exp-0827\nmodels/gemini-1.5-flash-latest\nmodels/gemini-1.5-flash-001\nmodels/gemini-1.5-flash-001-tuning\nmodels/gemini-1.5-flash\nmodels/gemini-1.5-flash-exp-0827\nmodels/gemini-1.5-flash-002\nmodels/gemini-1.5-flash-8b\nmodels/gemini-1.5-flash-8b-001\nmodels/gemini-1.5-flash-8b-latest\nmodels/gemini-1.5-flash-8b-exp-0827\nmodels/gemini-1.5-flash-8b-exp-0924\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/aqa\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "  if model.name == 'models/gemini-1.5-flash':\n",
        "    print(model)\n",
        "    break"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:19:28.799502Z",
          "iopub.execute_input": "2024-11-12T17:19:28.799892Z",
          "iopub.status.idle": "2024-11-12T17:19:28.851915Z",
          "shell.execute_reply.started": "2024-11-12T17:19:28.799855Z",
          "shell.execute_reply": "2024-11-12T17:19:28.850522Z"
        },
        "id": "RDe30ts607yw",
        "outputId": "3b8292c5-d27e-4934-f260-1a055f7498c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Model(name='models/gemini-1.5-flash',\n      base_model_id='',\n      version='001',\n      display_name='Gemini 1.5 Flash',\n      description='Fast and versatile multimodal model for scaling across diverse tasks',\n      input_token_limit=1000000,\n      output_token_limit=8192,\n      supported_generation_methods=['generateContent', 'countTokens'],\n      temperature=1.0,\n      max_temperature=2.0,\n      top_p=0.95,\n      top_k=40)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "short_model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    generation_config=genai.GenerationConfig(max_output_tokens=200))\n",
        "\n",
        "response = short_model.generate_content('Write a 1000 word essay on the importance of olives in modern society.')\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:19:28.853319Z",
          "iopub.execute_input": "2024-11-12T17:19:28.853708Z",
          "iopub.status.idle": "2024-11-12T17:19:30.148646Z",
          "shell.execute_reply.started": "2024-11-12T17:19:28.853648Z",
          "shell.execute_reply": "2024-11-12T17:19:30.146806Z"
        },
        "id": "eKEfu-s_07yw",
        "outputId": "adc3d20b-4354-451e-c33d-7cbfb8051e7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "## The Olive: A Timeless Treasure in Modern Society\n\nFrom ancient Greek mythology to modern-day culinary trends, the olive tree and its fruit have held an enduring place in human history and culture. More than just a symbol of peace and prosperity, the olive, in its diverse forms, has profoundly impacted modern society, contributing to our health, economy, and even our very understanding of the world. \n\nThe olive's significance starts with its **nutritional prowess**. Rich in monounsaturated fats, particularly oleic acid, olives are considered a cornerstone of the Mediterranean diet, renowned for its positive impact on heart health. Studies have linked regular olive consumption to reduced risk of cardiovascular disease, strokes, and certain types of cancer. Beyond their heart-healthy fats, olives are a good source of vitamins E and K, antioxidants, and minerals like iron and calcium. Their versatility as a food source, whether enjoyed fresh, pickled, or pressed into oil, makes them a valuable component of a balanced diet\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "response = short_model.generate_content('Write a short poem on the importance of olives in modern society.')\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:19:30.150577Z",
          "iopub.execute_input": "2024-11-12T17:19:30.15101Z",
          "iopub.status.idle": "2024-11-12T17:19:30.954976Z",
          "shell.execute_reply.started": "2024-11-12T17:19:30.150967Z",
          "shell.execute_reply": "2024-11-12T17:19:30.953527Z"
        },
        "id": "k4o3vsMW07yw",
        "outputId": "8f0796cd-649f-4d06-b762-ca028d8f8fa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "A tiny fruit, a briny taste,\nOn tables spread, a welcome haste.\nFrom ancient groves, a story told,\nOf sun-kissed lands, and stories old.\n\nA humble olive, small and green,\nIn salads bright, a vibrant scene.\nA pungent oil, a savory treat,\nFor dips and dressings, oh so sweet.\n\nFrom savory bites to calming balm,\nA treasure shared, a calming calm.\nA symbol strong, of life and peace,\nThe olive branch, a world's release. \n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "high_temp_model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    generation_config=genai.GenerationConfig(temperature=2.0))\n",
        "\n",
        "\n",
        "# When running lots of queries, it's a good practice to use a retry policy so your code\n",
        "# automatically retries when hitting Resource Exhausted (quota limit) errors.\n",
        "retry_policy = {\n",
        "    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n",
        "}\n",
        "\n",
        "for _ in range(5):\n",
        "  response = high_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n",
        "                                              request_options=retry_policy)\n",
        "  if response.parts:\n",
        "    print(response.text, '-' * 25)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:19:30.956949Z",
          "iopub.execute_input": "2024-11-12T17:19:30.957492Z",
          "iopub.status.idle": "2024-11-12T17:19:32.179682Z",
          "shell.execute_reply.started": "2024-11-12T17:19:30.957431Z",
          "shell.execute_reply": "2024-11-12T17:19:32.178305Z"
        },
        "id": "-voszu5L07yw",
        "outputId": "2c972bec-4c00-49f3-c067-6a22c24f711f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Purple. \n -------------------------\nPurple \n -------------------------\nPurple. \n -------------------------\nTeal. \n -------------------------\nPurple. \n -------------------------\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "low_temp_model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    generation_config=genai.GenerationConfig(temperature=0.0))\n",
        "\n",
        "for _ in range(5):\n",
        "  response = low_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n",
        "                                             request_options=retry_policy)\n",
        "  if response.parts:\n",
        "    print(response.text, '-' * 25)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:19:32.181439Z",
          "iopub.execute_input": "2024-11-12T17:19:32.181858Z",
          "iopub.status.idle": "2024-11-12T17:19:33.372423Z",
          "shell.execute_reply.started": "2024-11-12T17:19:32.181814Z",
          "shell.execute_reply": "2024-11-12T17:19:33.370872Z"
        },
        "id": "e8jwsqq607yw",
        "outputId": "55fc4b68-0a73-4712-ce79-a65a36d365ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Purple \n -------------------------\nPurple \n -------------------------\nPurple \n -------------------------\nPurple \n -------------------------\nPurple \n -------------------------\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        # These are the default values for gemini-1.5-flash-001.\n",
        "        temperature=1.0,\n",
        "        top_k=64,\n",
        "        top_p=0.95,\n",
        "    ))\n",
        "\n",
        "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
        "response = model.generate_content(story_prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:19:33.374116Z",
          "iopub.execute_input": "2024-11-12T17:19:33.3745Z",
          "iopub.status.idle": "2024-11-12T17:19:42.859173Z",
          "shell.execute_reply.started": "2024-11-12T17:19:33.374459Z",
          "shell.execute_reply": "2024-11-12T17:19:42.857505Z"
        },
        "id": "0mxB9CpY07yx",
        "outputId": "873aa445-310d-44aa-dc78-8e543595d179"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Bartholomew, a ginger tabby with a penchant for napping in sunbeams, had always been content with the comforts of home. His days were filled with chasing dust motes, batting at dangling toys, and demanding head scratches from his human, Amelia. But one Tuesday, a stray wind blew open the back door, and Bartholomew, ever the curious creature, found himself outside.\n\nThe world outside was a whirlwind of unfamiliar smells and sights. He sniffed the air, his whiskers twitching, and felt a thrill of excitement course through his ginger fur. This wasn't the manicured lawn he was used to; this was a sprawling jungle of weeds and wildflowers, teeming with life.\n\nA plump robin hopped across the path, its bright eyes sparkling. Bartholomew, momentarily forgetting his domesticated manners, sprang into a chase, the robin flitting away with a mocking chirp. He stalked through the tall grass, his senses on high alert, the scent of earth and wildflowers intoxicating.\n\nA rustle in the bushes startled him. He peered through the foliage, his green eyes narrowing. A tiny, scruffy kitten, its fur the color of soot, was cowering, its whiskers quivering. The kitten, barely bigger than his paw, looked up at him with wide, scared eyes.\n\nSomething stirred in Bartholomew's chest. He wasn't just a pampered house cat, he was a protector, a leader. He gently nudged the kitten, offering a reassuring purr. He knew he had to find a way to get it back home, away from the dangers of the wild.\n\nFollowing the sun, a beacon in the vast expanse of the unknown, Bartholomew led the kitten through the overgrown garden, past a boisterous dog with a wagging tail, and finally, to a familiar fence. He meowed insistently at the closed gate, his voice hoarse from the adventures of the day.\n\nAmelia, hearing the call, rushed to the gate, her heart pounding. She gasped, her eyes wide with worry, as she saw Bartholomew, his fur dusted with earth, and the tiny kitten tucked under his belly.\n\nBartholomew, the pampered house cat, had ventured into the unknown and returned a hero. He had not only tasted the thrill of adventure, but he had also discovered a strength he never knew he possessed, a strength born from a heart full of kindness and a love for his home. He knew now that he was more than just a cat; he was a protector, a leader, and an explorer. And he wouldn't trade his Tuesday adventure for anything. \n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=5,\n",
        "    ))\n",
        "\n",
        "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment: \"\"\"\n",
        "\n",
        "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:19:42.861287Z",
          "iopub.execute_input": "2024-11-12T17:19:42.861883Z",
          "iopub.status.idle": "2024-11-12T17:20:08.073473Z",
          "shell.execute_reply.started": "2024-11-12T17:19:42.861819Z",
          "shell.execute_reply": "2024-11-12T17:20:08.071788Z"
        },
        "id": "PWHOWG-G07yx",
        "outputId": "ac6fe6b2-46ef-4905-8a82-12280b42da12"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Sentiment: **POSITIVE**\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "class Sentiment(enum.Enum):\n",
        "    POSITIVE = \"positive\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "    NEGATIVE = \"negative\"\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        response_mime_type=\"text/x.enum\",\n",
        "        response_schema=Sentiment\n",
        "    ))\n",
        "\n",
        "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:20:08.074911Z",
          "iopub.execute_input": "2024-11-12T17:20:08.075442Z",
          "iopub.status.idle": "2024-11-12T17:20:08.336754Z",
          "shell.execute_reply.started": "2024-11-12T17:20:08.07538Z",
          "shell.execute_reply": "2024-11-12T17:20:08.335328Z"
        },
        "id": "37cuuf2K07yx",
        "outputId": "a79de0e4-7df8-4753-f949-354a7117e26b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "positive\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "odel = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=250,\n",
        "    ))\n",
        "\n",
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "customer_order = \"Give me a large with cheese & pineapple\"\n",
        "\n",
        "\n",
        "response = model.generate_content([few_shot_prompt, customer_order], request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:20:08.338552Z",
          "iopub.execute_input": "2024-11-12T17:20:08.339609Z",
          "iopub.status.idle": "2024-11-12T17:20:08.595888Z",
          "shell.execute_reply.started": "2024-11-12T17:20:08.339557Z",
          "shell.execute_reply": "2024-11-12T17:20:08.594136Z"
        },
        "id": "qYxX8aYA07yx",
        "outputId": "ccea97c4-274b-416f-a570-b5f470666029"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "negative\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import typing_extensions as typing\n",
        "\n",
        "class PizzaOrder(typing.TypedDict):\n",
        "    size: str\n",
        "    ingredients: list[str]\n",
        "    type: str\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=PizzaOrder,\n",
        "    ))\n",
        "\n",
        "response = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:20:08.597574Z",
          "iopub.execute_input": "2024-11-12T17:20:08.598014Z",
          "iopub.status.idle": "2024-11-12T17:20:08.978281Z",
          "shell.execute_reply.started": "2024-11-12T17:20:08.597965Z",
          "shell.execute_reply": "2024-11-12T17:20:08.976794Z"
        },
        "id": "YVNCkqeJ07yx",
        "outputId": "8e7be392-934f-4bbd-fbe8-f104678b95b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert\"}\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "am 20 years old. How old is my partner? Return the answer immediately.\"\"\"\n",
        "\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "response = model.generate_content(prompt, request_options=retry_policy)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:20:08.979822Z",
          "iopub.execute_input": "2024-11-12T17:20:08.980197Z",
          "iopub.status.idle": "2024-11-12T17:20:09.241211Z",
          "shell.execute_reply.started": "2024-11-12T17:20:08.980159Z",
          "shell.execute_reply": "2024-11-12T17:20:09.239972Z"
        },
        "id": "oPNYLvQ407yx",
        "outputId": "cd354021-67c7-453f-fa13-091041962c5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "52 \n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
        "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:20:09.242621Z",
          "iopub.execute_input": "2024-11-12T17:20:09.243038Z",
          "iopub.status.idle": "2024-11-12T17:20:09.865269Z",
          "shell.execute_reply.started": "2024-11-12T17:20:09.242996Z",
          "shell.execute_reply": "2024-11-12T17:20:09.863768Z"
        },
        "id": "4x-H8yRs07yy",
        "outputId": "c147753d-ca2e-4aff-b50c-67e2535887b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Here's how to solve this:\n\n* **When you were 4:** Your partner was 3 times your age, which is 4 * 3 = 12 years old.\n* **Age difference:** This means your partner is 12 - 4 = 8 years older than you.\n* **Current age:** Since you are now 20, your partner is still 8 years older, making them 20 + 8 = 28 years old. \n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_instructions = \"\"\"\n",
        "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "     will return some similar entities to search and you can try to search the information from those topics.\n",
        " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "     so keep your searches short.\n",
        " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "\"\"\"\n",
        "\n",
        "example1 = \"\"\"Question\n",
        "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "Thought 1\n",
        "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "\n",
        "Action 1\n",
        "<search>Milhouse</search>\n",
        "\n",
        "Observation 1\n",
        "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "Thought 2\n",
        "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "Action 2\n",
        "<lookup>named after</lookup>\n",
        "\n",
        "Observation 2\n",
        "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "Thought 3\n",
        "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "Action 3\n",
        "<finish>Richard Nixon</finish>\n",
        "\"\"\"\n",
        "\n",
        "example2 = \"\"\"Question\n",
        "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "\n",
        "Thought 1\n",
        "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "Action 1\n",
        "<search>Colorado orogeny</search>\n",
        "\n",
        "Observation 1\n",
        "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "Thought 2\n",
        "It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "Action 2\n",
        "<lookup>eastern sector</lookup>\n",
        "\n",
        "Observation 2\n",
        "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "Thought 3\n",
        "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "Action 3\n",
        "<search>High Plains</search>\n",
        "\n",
        "Observation 3\n",
        "High Plains refers to one of two distinct land regions\n",
        "\n",
        "Thought 4\n",
        "I need to instead search High Plains (United States).\n",
        "\n",
        "Action 4\n",
        "<search>High Plains (United States)</search>\n",
        "\n",
        "Observation 4\n",
        "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "\n",
        "Thought 5\n",
        "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "Action 5\n",
        "<finish>1,800 to 7,000 ft</finish>\n",
        "\"\"\"\n",
        "\n",
        "# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:20:09.86677Z",
          "iopub.execute_input": "2024-11-12T17:20:09.867142Z",
          "iopub.status.idle": "2024-11-12T17:20:09.874794Z",
          "shell.execute_reply.started": "2024-11-12T17:20:09.867105Z",
          "shell.execute_reply": "2024-11-12T17:20:09.873561Z"
        },
        "id": "wj5-mGd907yy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Question\n",
        "Who was the youngest author listed on the transformers NLP paper?\n",
        "\"\"\"\n",
        "\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "react_chat = model.start_chat()\n",
        "\n",
        "# You will perform the Action, so generate up to, but not including, the Observation.\n",
        "config = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n",
        "\n",
        "resp = react_chat.send_message(\n",
        "    [model_instructions, example1, example2, question],\n",
        "    generation_config=config,\n",
        "    request_options=retry_policy)\n",
        "print(resp.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:20:09.87628Z",
          "iopub.execute_input": "2024-11-12T17:20:09.876651Z",
          "iopub.status.idle": "2024-11-12T17:20:12.125841Z",
          "shell.execute_reply.started": "2024-11-12T17:20:09.876612Z",
          "shell.execute_reply": "2024-11-12T17:20:12.124413Z"
        },
        "id": "mp0dakFW07yy",
        "outputId": "f1e41691-fbfa-465d-92a6-0465b9523fa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Thought 1\nI need to find the transformers NLP paper and then look for the youngest author listed.\n\nAction 1\n<search>transformers NLP paper</search>\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "observation = \"\"\"Observation 1\n",
        "[1706.03762] Attention Is All You Need\n",
        "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
        "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
        "\"\"\"\n",
        "resp = react_chat.send_message(observation, generation_config=config, request_options=retry_policy)\n",
        "print(resp.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:20:12.127431Z",
          "iopub.execute_input": "2024-11-12T17:20:12.127865Z",
          "iopub.status.idle": "2024-11-12T17:20:12.590479Z",
          "shell.execute_reply.started": "2024-11-12T17:20:12.127819Z",
          "shell.execute_reply": "2024-11-12T17:20:12.588624Z"
        },
        "id": "HEp-SerM07yy",
        "outputId": "b48a9ad8-409b-4302-bd18-55d139e712cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Thought 2\nThe observation lists all the authors but doesn't indicate their ages. I need to find information about the ages of the authors.\n\nAction 2\n<search>Ashish Vaswani age</search> \n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=1024,\n",
        "    ))\n",
        "\n",
        "# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\n",
        "code_prompt = \"\"\"\n",
        "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(code_prompt, request_options=retry_policy)\n",
        "Markdown(response.text)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:20:12.591789Z",
          "iopub.execute_input": "2024-11-12T17:20:12.5923Z",
          "iopub.status.idle": "2024-11-12T17:20:12.991314Z",
          "shell.execute_reply.started": "2024-11-12T17:20:12.592245Z",
          "shell.execute_reply": "2024-11-12T17:20:12.989887Z"
        },
        "id": "LKc5F4ue07yy",
        "outputId": "3792384e-4434-4d4f-abb8-d44c588b7e1f"
      },
      "outputs": [
        {
          "execution_count": 117,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "```python\ndef factorial(n):\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n```"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    tools='code_execution',)\n",
        "\n",
        "code_exec_prompt = \"\"\"\n",
        "Calculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you count them all.\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(code_exec_prompt, request_options=retry_policy)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:20:12.993897Z",
          "iopub.execute_input": "2024-11-12T17:20:12.994419Z",
          "iopub.status.idle": "2024-11-12T17:20:14.401106Z",
          "shell.execute_reply.started": "2024-11-12T17:20:12.99436Z",
          "shell.execute_reply": "2024-11-12T17:20:14.399604Z"
        },
        "id": "v1iFxr4A07yy",
        "outputId": "9573fd4a-536c-4a1d-cc5f-8a8cff82a65e"
      },
      "outputs": [
        {
          "execution_count": 118,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "I will calculate the sum of the first 14 odd prime numbers.\n\nFirst, I need to identify the first 14 odd prime numbers. A prime number is a whole number greater than 1 that has only two divisors: 1 and itself. An odd number is any integer that is not divisible by 2.\n\nHere are the first 14 odd primes: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43.\n\nNow, I need to calculate their sum.\n\n\n``` python\nsum = 2 + 3 + 5 + 7 + 11 + 13 + 17 + 19 + 23 + 29 + 31 + 37 + 41 + 43\nprint(f'sum = {sum}')\n\n```\n```\nsum = 281\n\n```\nThe sum of the first 14 odd prime numbers is 281. \n"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy\n",
        "\n",
        "primes = sympy.primerange(0, 100)\n",
        "odd_primes = [p for p in primes if p % 2 != 0]\n",
        "sum_of_primes = sum(odd_primes[:14])\n",
        "print(f'{sum_of_primes=}')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:20:14.402649Z",
          "iopub.execute_input": "2024-11-12T17:20:14.403105Z",
          "iopub.status.idle": "2024-11-12T17:20:14.411199Z",
          "shell.execute_reply.started": "2024-11-12T17:20:14.403054Z",
          "shell.execute_reply": "2024-11-12T17:20:14.409773Z"
        },
        "id": "B_fWlPT_07yy",
        "outputId": "361171f6-56e1-4e68-8a78-19084dee9a71"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "sum_of_primes=326\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for part in response.candidates[0].content.parts:\n",
        "  print(part)\n",
        "  print(\"-----\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:20:14.417295Z",
          "iopub.execute_input": "2024-11-12T17:20:14.417673Z",
          "iopub.status.idle": "2024-11-12T17:20:14.428502Z",
          "shell.execute_reply.started": "2024-11-12T17:20:14.417637Z",
          "shell.execute_reply": "2024-11-12T17:20:14.427008Z"
        },
        "id": "KabDJPfi07yy",
        "outputId": "f8c0bced-76ee-4531-f86b-655c0e7a7dd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "text: \"I will calculate the sum of the first 14 odd prime numbers.\\n\\nFirst, I need to identify the first 14 odd prime numbers. A prime number is a whole number greater than 1 that has only two divisors: 1 and itself. An odd number is any integer that is not divisible by 2.\\n\\nHere are the first 14 odd primes: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43.\\n\\nNow, I need to calculate their sum.\\n\\n\"\n\n-----\nexecutable_code {\n  language: PYTHON\n  code: \"\\nsum = 2 + 3 + 5 + 7 + 11 + 13 + 17 + 19 + 23 + 29 + 31 + 37 + 41 + 43\\nprint(f\\'sum = {sum}\\')\\n\"\n}\n\n-----\ncode_execution_result {\n  outcome: OUTCOME_OK\n  output: \"sum = 281\\n\"\n}\n\n-----\ntext: \"The sum of the first 14 odd prime numbers is 281. \\n\"\n\n-----\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for part in response.candidates[0].content.parts:\n",
        "  print(part)\n",
        "  print(\"-----\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T17:20:14.430378Z",
          "iopub.execute_input": "2024-11-12T17:20:14.430784Z",
          "iopub.status.idle": "2024-11-12T17:20:14.446063Z",
          "shell.execute_reply.started": "2024-11-12T17:20:14.430731Z",
          "shell.execute_reply": "2024-11-12T17:20:14.44474Z"
        },
        "id": "A7PZVlqb07yz",
        "outputId": "9ace8a57-6f0e-45cd-cc62-8d2b09991d62"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "text: \"I will calculate the sum of the first 14 odd prime numbers.\\n\\nFirst, I need to identify the first 14 odd prime numbers. A prime number is a whole number greater than 1 that has only two divisors: 1 and itself. An odd number is any integer that is not divisible by 2.\\n\\nHere are the first 14 odd primes: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43.\\n\\nNow, I need to calculate their sum.\\n\\n\"\n\n-----\nexecutable_code {\n  language: PYTHON\n  code: \"\\nsum = 2 + 3 + 5 + 7 + 11 + 13 + 17 + 19 + 23 + 29 + 31 + 37 + 41 + 43\\nprint(f\\'sum = {sum}\\')\\n\"\n}\n\n-----\ncode_execution_result {\n  outcome: OUTCOME_OK\n  output: \"sum = 281\\n\"\n}\n\n-----\ntext: \"The sum of the first 14 odd prime numbers is 281. \\n\"\n\n-----\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}